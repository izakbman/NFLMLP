{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9246, 77) (1156, 77) (9246,) (1156,)\n"
     ]
    }
   ],
   "source": [
    "X_train = joblib.load('X_train_transformed.pkl')\n",
    "X_valid = joblib.load('X_valid_transformed.pkl')\n",
    "X_test = joblib.load('X_test_transformed.pkl')\n",
    "\n",
    "y_train = joblib.load('y_train.pkl')\n",
    "y_valid = joblib.load('y_valid.pkl')\n",
    "y_test = joblib.load('y_test.pkl')\n",
    "transformer = joblib.load('transformer.pkl')\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic Linear Regression MSE: 95.31188464577222\n",
      "Basic Linear Regression R2: 0.014760718894951785\n"
     ]
    }
   ],
   "source": [
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "\n",
    "mse_lr = mean_squared_error(y_test, y_pred_lr)\n",
    "r2_lr = r2_score(y_test, y_pred_lr)\n",
    "\n",
    "\n",
    "print(\"Basic Linear Regression MSE:\", mse_lr); print(\"Basic Linear Regression R2:\", r2_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params for KRR Polynomial: {'alpha': 10, 'degree': 2, 'kernel': 'polynomial'}\n",
      "Kernel Ridge Regression (Polynomial) MSE: 95.56678483703487\n",
      "Kernel Ridge Regression (Polynomial) R2: 0.012125814736603568\n"
     ]
    }
   ],
   "source": [
    "param_grid_poly = {'kernel': ['polynomial'], 'degree': [2, 3, 4], 'alpha': [0.1, 1, 10]}\n",
    "\n",
    "krr_poly_grid = GridSearchCV(KernelRidge(), param_grid_poly, cv=3, scoring='neg_mean_squared_error'); krr_poly_grid.fit(X_valid, y_valid)\n",
    "best_params_poly = krr_poly_grid.best_params_\n",
    "best_krr_poly = KernelRidge(**best_params_poly); best_krr_poly.fit(X_train, y_train)\n",
    "\n",
    "y_pred_krr_poly = best_krr_poly.predict(X_test)\n",
    "mse_krr_poly = mean_squared_error(y_test, y_pred_krr_poly); r2_krr_poly = r2_score(y_test, y_pred_krr_poly)\n",
    "\n",
    "print(\"Best params for KRR Polynomial:\", best_params_poly); print(\"Kernel Ridge Regression (Polynomial) MSE:\", mse_krr_poly); print(\"Kernel Ridge Regression (Polynomial) R2:\", r2_krr_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Kernel Ridge Regression (RBF): {'alpha': 10, 'coef0': 0, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "Kernel Ridge Regression (RBF) MSE: 96.44069512794619\n",
      "Kernel Ridge Regression (RBF) R2: 0.0030922010380840526\n"
     ]
    }
   ],
   "source": [
    "param_grid_rbf = {\n",
    "    'kernel': ['rbf'],\n",
    "    'alpha': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'gamma': [0.0001, 0.001, 0.01, 0.1, 1, 10],\n",
    "    'coef0': [0, 0.1, 1]\n",
    "}\n",
    "\n",
    "krr_rbf_grid = GridSearchCV(KernelRidge(), param_grid_rbf, cv=3, scoring='neg_mean_squared_error')\n",
    "\n",
    "krr_rbf_grid.fit(X_valid, y_valid)\n",
    "\n",
    "best_params_rbf = krr_rbf_grid.best_params_\n",
    "best_krr_rbf = KernelRidge(**best_params_rbf)\n",
    "best_krr_rbf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_krr_rbf = best_krr_rbf.predict(X_test)\n",
    "mse_krr_rbf = mean_squared_error(y_test, y_pred_krr_rbf)\n",
    "r2_krr_rbf = r2_score(y_test, y_pred_krr_rbf)\n",
    "print(\"Best parameters for Kernel Ridge Regression (RBF):\", best_params_rbf)\n",
    "print(\"Kernel Ridge Regression (RBF) MSE:\", mse_krr_rbf)\n",
    "print(\"Kernel Ridge Regression (RBF) R2:\", r2_krr_rbf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Basic SVM (Linear Kernel)...\n",
      "Basic SVM (Linear) MSE: 100.58736648647843\n",
      "Basic SVM (Linear) R²: -0.03977195513134735\n",
      "\n",
      "Training SVM with Polynomial Kernel...\n",
      "Best Polynomial Kernel Params: {'C': 1, 'coef0': 0, 'degree': 2, 'epsilon': 0.5, 'kernel': 'poly'}\n",
      "SVM (Polynomial Kernel) MSE: 101.62426073990372\n",
      "SVM (Polynomial Kernel) R²: -0.0504903346138581\n",
      "\n",
      "Training SVM with RBF Kernel...\n",
      "Best RBF Kernel Params: {'C': 10, 'epsilon': 0.5, 'gamma': 1, 'kernel': 'rbf'}\n",
      "SVM (RBF Kernel) MSE: 98.30144036885515\n",
      "SVM (RBF Kernel) R²: -0.016142328950345064\n",
      "\n",
      "Summary of Model Performance:\n",
      "Basic SVM (Linear) MSE: 100.5874, R²: -0.0398\n",
      "SVM (Polynomial) MSE: 101.6243, R²: -0.0505\n",
      "SVM (RBF) MSE: 98.3014, R²: -0.0161\n"
     ]
    }
   ],
   "source": [
    "# 1. Basic SVM (Linear Kernel) - Regression\n",
    "print(\"Training Basic SVM (Linear Kernel)...\")\n",
    "svm_basic = SVR(kernel='linear')\n",
    "svm_basic.fit(X_train, y_train)\n",
    "\n",
    "y_pred_basic = svm_basic.predict(X_test)\n",
    "mse_basic = mean_squared_error(y_test, y_pred_basic)\n",
    "r2_basic = r2_score(y_test, y_pred_basic)\n",
    "print(\"Basic SVM (Linear) MSE:\", mse_basic)\n",
    "print(\"Basic SVM (Linear) R²:\", r2_basic)\n",
    "\n",
    "# 2. SVM with Polynomial Kernel (Kernel Trick) - Regression\n",
    "print(\"\\nTraining SVM with Polynomial Kernel...\")\n",
    "param_grid_poly = {\n",
    "    'kernel': ['poly'],\n",
    "    'degree': [2, 3],\n",
    "    'C': [0.1, 1, 10],\n",
    "    'coef0': [0, 1],\n",
    "    'epsilon': [0.1, 0.5]\n",
    "}\n",
    "\n",
    "svm_poly_grid = GridSearchCV(SVR(), param_grid_poly, cv=2, scoring='neg_mean_squared_error')\n",
    "svm_poly_grid.fit(X_valid, y_valid)\n",
    "\n",
    "best_params_poly = svm_poly_grid.best_params_\n",
    "print(\"Best Polynomial Kernel Params:\", best_params_poly)\n",
    "best_svm_poly = SVR(**best_params_poly)\n",
    "best_svm_poly.fit(X_train, y_train)\n",
    "\n",
    "y_pred_poly = best_svm_poly.predict(X_test)\n",
    "mse_poly = mean_squared_error(y_test, y_pred_poly)\n",
    "r2_poly = r2_score(y_test, y_pred_poly)\n",
    "print(\"SVM (Polynomial Kernel) MSE:\", mse_poly)\n",
    "print(\"SVM (Polynomial Kernel) R²:\", r2_poly)\n",
    "\n",
    "# 3. SVM with RBF Kernel (Kernel Trick) - Regression\n",
    "print(\"\\nTraining SVM with RBF Kernel...\")\n",
    "param_grid_rbf = {\n",
    "    'kernel': ['rbf'],\n",
    "    'C': [0.1, 1, 10],\n",
    "    'gamma': [0.001, 0.1, 1],\n",
    "    'epsilon': [0.1, 0.5]\n",
    "}\n",
    "\n",
    "svm_rbf_grid = GridSearchCV(SVR(), param_grid_rbf, cv=2, scoring='neg_mean_squared_error')\n",
    "svm_rbf_grid.fit(X_valid, y_valid)\n",
    "\n",
    "best_params_rbf = svm_rbf_grid.best_params_\n",
    "print(\"Best RBF Kernel Params:\", best_params_rbf)\n",
    "best_svm_rbf = SVR(**best_params_rbf)\n",
    "best_svm_rbf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rbf = best_svm_rbf.predict(X_test)\n",
    "mse_rbf = mean_squared_error(y_test, y_pred_rbf)\n",
    "r2_rbf = r2_score(y_test, y_pred_rbf)\n",
    "print(\"SVM (RBF Kernel) MSE:\", mse_rbf)\n",
    "print(\"SVM (RBF Kernel) R²:\", r2_rbf)\n",
    "\n",
    "print(\"\\nSummary of Model Performance:\")\n",
    "print(f\"Basic SVM (Linear) MSE: {mse_basic:.4f}, R²: {r2_basic:.4f}\")\n",
    "print(f\"SVM (Polynomial) MSE: {mse_poly:.4f}, R²: {r2_poly:.4f}\")\n",
    "print(f\"SVM (RBF) MSE: {mse_rbf:.4f}, R²: {r2_rbf:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Decision Tree Regressor...\n",
      "Best Decision Tree Params: {'max_depth': 3, 'min_samples_leaf': 4, 'min_samples_split': 10}\n",
      "Decision Tree MSE: 95.26156791242974\n",
      "Decision Tree R²: 0.015280843141473155\n",
      "\n",
      "Training Random Forest Regressor...\n",
      "Best Random Forest Params: {'max_depth': 5, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 200}\n",
      "Random Forest MSE: 95.18085905916578\n",
      "Random Forest R²: 0.016115130836697245\n",
      "\n",
      "Baseline MSE (mean prediction): 97.030276816609\n",
      "\n",
      "Summary of Model Performance:\n",
      "Decision Tree MSE: 95.2616, R²: 0.0153\n",
      "Random Forest MSE: 95.1809, R²: 0.0161\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Decision Tree Regressor...\")\n",
    "param_grid_dt = {\n",
    "    'max_depth': [3, 5, 10, None],          # Maximum depth of the tree\n",
    "    'min_samples_split': [2, 5, 10],        # Minimum samples to split a node\n",
    "    'min_samples_leaf': [1, 2, 4]           # Minimum samples at a leaf node\n",
    "}\n",
    "\n",
    "dt_grid = GridSearchCV(DecisionTreeRegressor(random_state=42), param_grid_dt, cv=3, scoring='neg_mean_squared_error')\n",
    "dt_grid.fit(X_valid, y_valid)  # Tune on validation set\n",
    "\n",
    "best_params_dt = dt_grid.best_params_\n",
    "print(\"Best Decision Tree Params:\", best_params_dt)\n",
    "best_dt = DecisionTreeRegressor(**best_params_dt, random_state=42)\n",
    "best_dt.fit(X_train, y_train)\n",
    "\n",
    "y_pred_dt = best_dt.predict(X_test)\n",
    "mse_dt = mean_squared_error(y_test, y_pred_dt)\n",
    "r2_dt = r2_score(y_test, y_pred_dt)\n",
    "print(\"Decision Tree MSE:\", mse_dt)\n",
    "print(\"Decision Tree R²:\", r2_dt)\n",
    "\n",
    "print(\"\\nTraining Random Forest Regressor...\")\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200],         # Number of trees\n",
    "    'max_depth': [5, 10, None],             # Maximum depth of each tree\n",
    "    'min_samples_split': [2, 5],            # Minimum samples to split a node\n",
    "    'min_samples_leaf': [1, 2]              # Minimum samples at a leaf node\n",
    "}\n",
    "\n",
    "rf_grid = GridSearchCV(RandomForestRegressor(random_state=42, n_jobs=-1), param_grid_rf, cv=3, scoring='neg_mean_squared_error')\n",
    "rf_grid.fit(X_valid, y_valid)\n",
    "\n",
    "best_params_rf = rf_grid.best_params_\n",
    "print(\"Best Random Forest Params:\", best_params_rf)\n",
    "best_rf = RandomForestRegressor(**best_params_rf, random_state=42, n_jobs=-1)\n",
    "best_rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = best_rf.predict(X_test)\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "print(\"Random Forest MSE:\", mse_rf)\n",
    "print(\"Random Forest R²:\", r2_rf)\n",
    "\n",
    "baseline_mse = mean_squared_error(y_test, np.full_like(y_test, y_train.mean()))\n",
    "print(\"\\nBaseline MSE (mean prediction):\", baseline_mse)\n",
    "\n",
    "print(\"\\nSummary of Model Performance:\")\n",
    "print(f\"Decision Tree MSE: {mse_dt:.4f}, R²: {r2_dt:.4f}\")\n",
    "print(f\"Random Forest MSE: {mse_rf:.4f}, R²: {r2_rf:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 102 - Train batches: 289, Valid batches: 37, Test batches: 37\n",
      "Seed 203 - Train batches: 289, Valid batches: 37, Test batches: 37\n",
      "Seed 305 - Train batches: 289, Valid batches: 37, Test batches: 37\n",
      "Seed 405 - Train batches: 289, Valid batches: 37, Test batches: 37\n",
      "Seed 506 - Train batches: 289, Valid batches: 37, Test batches: 37\n",
      "Seed 607 - Train batches: 289, Valid batches: 37, Test batches: 37\n",
      "\n",
      "Training neural net with seed 102...\n",
      "Seed 102, Epoch [20/100], Train Loss: 79.9207, Val Loss: 79.6967\n",
      "Seed 102, Epoch [40/100], Train Loss: 78.3902, Val Loss: 79.1503\n",
      "Early stopping at epoch 49 for seed 102\n",
      "Seed 102 - Test MSE: 97.4210, Test R²: -0.0070\n",
      "\n",
      "Training neural net with seed 203...\n",
      "Seed 203, Epoch [20/100], Train Loss: 79.5282, Val Loss: 83.0422\n",
      "Seed 203, Epoch [40/100], Train Loss: 78.6688, Val Loss: 82.7202\n",
      "Early stopping at epoch 47 for seed 203\n",
      "Seed 203 - Test MSE: 91.1573, Test R²: -0.0103\n",
      "\n",
      "Training neural net with seed 305...\n",
      "Seed 305, Epoch [20/100], Train Loss: 79.1893, Val Loss: 80.8893\n",
      "Seed 305, Epoch [40/100], Train Loss: 78.4359, Val Loss: 80.4447\n",
      "Early stopping at epoch 50 for seed 305\n",
      "Seed 305 - Test MSE: 95.4704, Test R²: 0.0064\n",
      "\n",
      "Training neural net with seed 405...\n",
      "Seed 405, Epoch [20/100], Train Loss: 79.9084, Val Loss: 101.0800\n",
      "Seed 405, Epoch [40/100], Train Loss: 78.4962, Val Loss: 100.2676\n",
      "Seed 405, Epoch [60/100], Train Loss: 77.7510, Val Loss: 99.9858\n",
      "Early stopping at epoch 60 for seed 405\n",
      "Seed 405 - Test MSE: 72.1260, Test R²: 0.0130\n",
      "\n",
      "Training neural net with seed 506...\n",
      "Seed 506, Epoch [20/100], Train Loss: 78.8215, Val Loss: 77.4432\n",
      "Early stopping at epoch 38 for seed 506\n",
      "Seed 506 - Test MSE: 104.1055, Test R²: -0.0137\n",
      "\n",
      "Training neural net with seed 607...\n",
      "Seed 607, Epoch [20/100], Train Loss: 82.3176, Val Loss: 88.5140\n",
      "Seed 607, Epoch [40/100], Train Loss: 80.7690, Val Loss: 87.7014\n",
      "Early stopping at epoch 47 for seed 607\n",
      "Seed 607 - Test MSE: 66.7503, Test R²: 0.0141\n",
      "\n",
      "Neural Network Average across 6 seeds - MSE: 87.8384, R²: 0.0004\n"
     ]
    }
   ],
   "source": [
    "class PlayOutcomeNet(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(PlayOutcomeNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "random_seeds = [102, 203, 305, 405, 506, 607]\n",
    "batch_size = 32\n",
    "\n",
    "train_loaders = {}\n",
    "valid_loaders = {}\n",
    "test_loaders = {}\n",
    "\n",
    "for seed in random_seeds:\n",
    "    X_train_tensor = torch.load(f'X_train_seed{seed}.pt')\n",
    "    y_train_tensor = torch.load(f'y_train_seed{seed}.pt')\n",
    "    X_valid_tensor = torch.load(f'X_valid_seed{seed}.pt')\n",
    "    y_valid_tensor = torch.load(f'y_valid_seed{seed}.pt')\n",
    "    X_test_tensor = torch.load(f'X_test_seed{seed}.pt')\n",
    "    y_test_tensor = torch.load(f'y_test_seed{seed}.pt')\n",
    "\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    valid_dataset = TensorDataset(X_valid_tensor, y_valid_tensor)\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "    train_loaders[seed] = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    valid_loaders[seed] = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loaders[seed] = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    print(f\"Seed {seed} - Train batches: {len(train_loaders[seed])}, \"\n",
    "          f\"Valid batches: {len(valid_loaders[seed])}, Test batches: {len(test_loaders[seed])}\")\n",
    "\n",
    "num_epochs = 100\n",
    "results = {'mse': [], 'r2': []}\n",
    "\n",
    "for seed in random_seeds:\n",
    "    print(f\"\\nTraining neural net with seed {seed}...\")\n",
    "    torch.manual_seed(seed)\n",
    "    model = PlayOutcomeNet(input_size=X_train_tensor.shape[1])\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-3)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 10\n",
    "    counter = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, targets in train_loaders[seed]:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(valid_loaders[seed].dataset.tensors[0])\n",
    "            val_loss = criterion(val_outputs, valid_loaders[seed].dataset.tensors[1])\n",
    "        \n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            print(f\"Seed {seed}, Epoch [{epoch+1}/{num_epochs}], \"\n",
    "                  f\"Train Loss: {running_loss/len(train_loaders[seed]):.4f}, Val Loss: {val_loss.item():.4f}\")\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = model.state_dict()\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1} for seed {seed}\")\n",
    "            model.load_state_dict(best_model_state)\n",
    "            break\n",
    "\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loaders[seed]:\n",
    "            outputs = model(inputs)\n",
    "            y_pred.extend(outputs.numpy().flatten())\n",
    "            y_true.extend(targets.numpy().flatten())\n",
    "    \n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    results['mse'].append(mse)\n",
    "    results['r2'].append(r2)\n",
    "    print(f\"Seed {seed} - Test MSE: {mse:.4f}, Test R²: {r2:.4f}\")\n",
    "\n",
    "avg_mse = np.mean(results['mse'])\n",
    "avg_r2 = np.mean(results['r2'])\n",
    "print(f\"\\nNeural Network Average across 6 seeds - MSE: {avg_mse:.4f}, R²: {avg_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 102 - Train batches: 289, Valid batches: 37, Test batches: 37\n",
      "Seed 203 - Train batches: 289, Valid batches: 37, Test batches: 37\n",
      "Seed 305 - Train batches: 289, Valid batches: 37, Test batches: 37\n",
      "Seed 405 - Train batches: 289, Valid batches: 37, Test batches: 37\n",
      "Seed 506 - Train batches: 289, Valid batches: 37, Test batches: 37\n",
      "Seed 607 - Train batches: 289, Valid batches: 37, Test batches: 37\n",
      "\n",
      "Training neural net with seed 102...\n",
      "Seed 102, Epoch [20/100], Train Loss: 79.9207, Val Loss: 79.6967\n",
      "Seed 102, Epoch [40/100], Train Loss: 78.3902, Val Loss: 79.1503\n",
      "Seed 102, Epoch [60/100], Train Loss: 76.7471, Val Loss: 79.3862\n",
      "Seed 102, Epoch [80/100], Train Loss: 76.7021, Val Loss: 79.8244\n",
      "Seed 102, Epoch [100/100], Train Loss: 76.1161, Val Loss: 80.2313\n",
      "Seed 102 - Test MSE: 97.2482, Test R²: -0.0053\n",
      "\n",
      "Training neural net with seed 203...\n",
      "Seed 203, Epoch [20/100], Train Loss: 79.5282, Val Loss: 83.0422\n",
      "Seed 203, Epoch [40/100], Train Loss: 78.6688, Val Loss: 82.7202\n",
      "Seed 203, Epoch [60/100], Train Loss: 77.8461, Val Loss: 82.9886\n",
      "Seed 203, Epoch [80/100], Train Loss: 76.7169, Val Loss: 83.5010\n",
      "Seed 203, Epoch [100/100], Train Loss: 75.7642, Val Loss: 83.8335\n",
      "Seed 203 - Test MSE: 91.9770, Test R²: -0.0194\n",
      "\n",
      "Training neural net with seed 305...\n",
      "Seed 305, Epoch [20/100], Train Loss: 79.1893, Val Loss: 80.8893\n",
      "Seed 305, Epoch [40/100], Train Loss: 78.4359, Val Loss: 80.4447\n",
      "Seed 305, Epoch [60/100], Train Loss: 77.2903, Val Loss: 80.6615\n",
      "Seed 305, Epoch [80/100], Train Loss: 77.2041, Val Loss: 80.9342\n",
      "Seed 305, Epoch [100/100], Train Loss: 76.1675, Val Loss: 81.1760\n",
      "Seed 305 - Test MSE: 95.6301, Test R²: 0.0048\n",
      "\n",
      "Training neural net with seed 405...\n",
      "Seed 405, Epoch [20/100], Train Loss: 79.9084, Val Loss: 101.0800\n",
      "Seed 405, Epoch [40/100], Train Loss: 78.4962, Val Loss: 100.2676\n",
      "Seed 405, Epoch [60/100], Train Loss: 77.7510, Val Loss: 99.9858\n",
      "Seed 405, Epoch [80/100], Train Loss: 77.4072, Val Loss: 100.1246\n",
      "Seed 405, Epoch [100/100], Train Loss: 76.1814, Val Loss: 100.3463\n",
      "Seed 405 - Test MSE: 71.9953, Test R²: 0.0147\n",
      "\n",
      "Training neural net with seed 506...\n",
      "Seed 506, Epoch [20/100], Train Loss: 78.8215, Val Loss: 77.4432\n",
      "Seed 506, Epoch [40/100], Train Loss: 76.9574, Val Loss: 77.1493\n",
      "Seed 506, Epoch [60/100], Train Loss: 76.1198, Val Loss: 77.2519\n",
      "Seed 506, Epoch [80/100], Train Loss: 75.1033, Val Loss: 77.2282\n",
      "Seed 506, Epoch [100/100], Train Loss: 74.8001, Val Loss: 77.2815\n",
      "Seed 506 - Test MSE: 104.0647, Test R²: -0.0133\n",
      "\n",
      "Training neural net with seed 607...\n",
      "Seed 607, Epoch [20/100], Train Loss: 82.3176, Val Loss: 88.5140\n",
      "Seed 607, Epoch [40/100], Train Loss: 80.7690, Val Loss: 87.7014\n",
      "Seed 607, Epoch [60/100], Train Loss: 79.5801, Val Loss: 88.1765\n",
      "Seed 607, Epoch [80/100], Train Loss: 79.2002, Val Loss: 88.3649\n",
      "Seed 607, Epoch [100/100], Train Loss: 77.7846, Val Loss: 88.7277\n",
      "Seed 607 - Test MSE: 67.1716, Test R²: 0.0079\n",
      "\n",
      "Neural Network Average across 6 seeds - MSE: 88.0145, R²: -0.0018\n"
     ]
    }
   ],
   "source": [
    "class PlayOutcomeNet(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(PlayOutcomeNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "random_seeds = [102, 203, 305, 405, 506, 607]\n",
    "batch_size = 32\n",
    "\n",
    "train_loaders = {}\n",
    "valid_loaders = {}\n",
    "test_loaders = {}\n",
    "\n",
    "for seed in random_seeds:\n",
    "    X_train_tensor = torch.load(f'X_train_seed{seed}.pt')\n",
    "    y_train_tensor = torch.load(f'y_train_seed{seed}.pt')\n",
    "    X_valid_tensor = torch.load(f'X_valid_seed{seed}.pt')\n",
    "    y_valid_tensor = torch.load(f'y_valid_seed{seed}.pt')\n",
    "    X_test_tensor = torch.load(f'X_test_seed{seed}.pt')\n",
    "    y_test_tensor = torch.load(f'y_test_seed{seed}.pt')\n",
    "\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    valid_dataset = TensorDataset(X_valid_tensor, y_valid_tensor)\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "    train_loaders[seed] = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    valid_loaders[seed] = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loaders[seed] = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    print(f\"Seed {seed} - Train batches: {len(train_loaders[seed])}, \"\n",
    "          f\"Valid batches: {len(valid_loaders[seed])}, Test batches: {len(test_loaders[seed])}\")\n",
    "\n",
    "num_epochs = 100\n",
    "results = {'mse': [], 'r2': []}\n",
    "\n",
    "for seed in random_seeds:\n",
    "    print(f\"\\nTraining neural net with seed {seed}...\")\n",
    "    torch.manual_seed(seed)\n",
    "    model = PlayOutcomeNet(input_size=X_train_tensor.shape[1])\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-3)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, targets in train_loaders[seed]:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(valid_loaders[seed].dataset.tensors[0])\n",
    "            val_loss = criterion(val_outputs, valid_loaders[seed].dataset.tensors[1])\n",
    "        \n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            print(f\"Seed {seed}, Epoch [{epoch+1}/{num_epochs}], \"\n",
    "                  f\"Train Loss: {running_loss/len(train_loaders[seed]):.4f}, Val Loss: {val_loss.item():.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loaders[seed]:\n",
    "            outputs = model(inputs)\n",
    "            y_pred.extend(outputs.numpy().flatten())\n",
    "            y_true.extend(targets.numpy().flatten())\n",
    "    \n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    results['mse'].append(mse)\n",
    "    results['r2'].append(r2)\n",
    "    print(f\"Seed {seed} - Test MSE: {mse:.4f}, Test R²: {r2:.4f}\")\n",
    "\n",
    "\n",
    "avg_mse = np.mean(results['mse'])\n",
    "avg_r2 = np.mean(results['r2'])\n",
    "print(f\"\\nNeural Network Average across 6 seeds - MSE: {avg_mse:.4f}, R²: {avg_r2:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
